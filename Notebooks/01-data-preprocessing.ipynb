{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Run this notebook to create `processed_dataset.csv` in `../data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_token_list(file_path):\n",
    "    \"\"\"Gets a list of tokens (words) from a file.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_text = f.read()\n",
    "    pattern = r\"[\\r\\n]*\\[BODY\\]:\\s*[\\r\\n]*\"\n",
    "    body = re.split(pattern, file_text)[1]\n",
    "    body = body.replace(\"\\n\", \"\")\n",
    "    body = body.lower()\n",
    "    translation_map = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    body = body.translate(translation_map)\n",
    "    tokens = body.split(\" \")\n",
    "    tokens = [token for token in tokens if token != \"\"]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Dataset contains input (a single original token) in the first column and the label (a single normalized token)\n",
    "dataset = np.zeros((0, 2))\n",
    "\n",
    "# Iterate over original and modernized documents from each century and add them to the dataset if the documents have the same number of tokens\n",
    "# This is a naive (but largely effective) attempt at word alignments\n",
    "centuries = [\"16th_century\", \"17th_century\", \"18th_century\", \"19th_century\"]\n",
    "for century in centuries:    \n",
    "    print(f\"Processing {century} documents...\")\n",
    "    original_files = glob.glob(f\"../data/post_scriptum_spanish/original/{century}/*.txt\")\n",
    "    modernized_files = glob.glob(f\"../data/post_scriptum_spanish/modernized/{century}/*.txt\")\n",
    "    for index, file in enumerate(original_files):\n",
    "        original_tokens = get_token_list(file)\n",
    "        modernized_tokens = get_token_list(modernized_files[index])\n",
    "        if len(original_tokens) == len(modernized_tokens):\n",
    "            new_data = np.column_stack((original_tokens, modernized_tokens))\n",
    "            dataset = np.append(dataset, new_data, axis=0)\n",
    "\n",
    "df = pd.DataFrame(dataset, columns=[\"Original\", \"Modernized\"])\n",
    "df.to_csv(r\"../data/processed_dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a4",
   "language": "python",
   "name": "a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
