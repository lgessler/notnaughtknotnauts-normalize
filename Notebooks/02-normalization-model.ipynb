{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics, optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "# Define unknown, pad, start, and end characters as Hindi letters since they won't be seen in the data\n",
    "UNK = 'अ'\n",
    "PAD = 'आ'\n",
    "START = 'श'\n",
    "END = 'स'\n",
    "\n",
    "dataset = pd.read_csv(r\"../data/processed_dataset.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Character Vocabulary\n",
    "\n",
    "1. Get two lists, one containing all of the characters used in the original documens and one containing all of the characters used in the modernized documents\n",
    "2. Add the special `UNK`, `PAD`, `START`, and `END` characters to each list.\n",
    "3. Create a vocabulary (i.e. a dictionary) mapping each character to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original tokens are in the first column and modernized tokens are in the second column\n",
    "original = list(dataset[:, 0])\n",
    "modernized = list(dataset[:, 1])\n",
    "\n",
    "input_characters = list(set((character for word in original for character in word)))\n",
    "input_characters += [UNK, PAD, START, END]\n",
    "input_characters = sorted(input_characters)\n",
    "input_vocab = {character:index for index, character in enumerate(input_characters)}\n",
    "\n",
    "labels_characters = list(set((character for word in modernized for character in word)))\n",
    "labels_characters += [UNK, PAD, START, END]\n",
    "labels_characters = sorted(labels_characters)\n",
    "labels_vocab = {character:index for index, character in enumerate(labels_characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization and Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequence(seq, vocab):\n",
    "    \"\"\"Takes a sequence of words and returns a sequence of integers.\"\"\"\n",
    "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
    "    return [vocab[tok] for tok in seq]\n",
    "\n",
    "\n",
    "def unvectorize_sequence(seq, vocab):\n",
    "    \"\"\"Takes a sequence of integers and returns a sequence of words.\"\"\"\n",
    "    vocab_words = list(vocab.keys())\n",
    "    return [vocab_words[i] for i in seq]\n",
    "\n",
    "\n",
    "def one_hot_encode_label(character, labels):\n",
    "    \"\"\"One-hot encodes a character.\"\"\"\n",
    "    vec = [1.0 if label == character else 0.0 for label in labels]\n",
    "    return np.array(vec)\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_length, pad_value):\n",
    "    \"\"\"Takes a batch of sequences of different lengths and pads them with the PAD character so that they are all the same length.\"\"\"\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        if len(sequence) < pad_length:\n",
    "            sequences[i] = sequence + ([pad_value] * (pad_length - len(sequence)))\n",
    "    return sequences\n",
    "\n",
    "def batch_generator(data, labels, vocab, labels_vocab, batch_size=1):\n",
    "    \"\"\"Generates a batch of samples for training.\"\"\"\n",
    "    while True:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for word, normalized_word in zip(data, labels):\n",
    "            word = word[0:10]\n",
    "            normalized_word = normalized_word[0:10]\n",
    "            word = START + word + END\n",
    "            normalized_word = START + normalized_word + END\n",
    "            batch_x.append(vectorize_sequence(word, vocab))\n",
    "            batch_y.append([one_hot_encode_label(character, labels_vocab) for character in normalized_word])\n",
    "            if len(batch_x) >= batch_size:\n",
    "                # Pad Sequences in batch to same length\n",
    "                pad_length = len(max(batch_x + batch_y, key=lambda x: len(x)))\n",
    "                batch_x = pad_sequences(batch_x, pad_length, vocab[PAD])\n",
    "                batch_y = pad_sequences(batch_y, pad_length, one_hot_encode_label(PAD, labels_vocab))\n",
    "                yield np.array(batch_x), np.array(batch_y)\n",
    "                batch_x = []\n",
    "                batch_y = []    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_vocab, labels_vocab, embedding_size, hidden_size, dropout):\n",
    "    \"\"\"Builds and returns a Keras model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(input_vocab.keys()), embedding_size))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Bidirectional(LSTM(hidden_size, return_sequences=True)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(TimeDistributed(Dense(len(labels_vocab.keys()), activation='softmax')))\n",
    "\n",
    "    adadelta = optimizers.Adadelta(clipnorm=1.0)\n",
    "    model.compile(optimizer=adadelta, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "305/304 [==============================] - 18s 59ms/step - loss: 1.3304 - acc: 0.6889\n",
      "Epoch 2/3\n",
      "305/304 [==============================] - 16s 54ms/step - loss: 0.7241 - acc: 0.8614\n",
      "Epoch 3/3\n",
      "305/304 [==============================] - 16s 54ms/step - loss: 0.6435 - acc: 0.8774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f66297e80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Split dataset into three parts: 80% training, 10% dev, 10% test\n",
    "train, dev, test = np.split(dataset, [math.floor(.8*len(dataset)), math.floor(.9*len(dataset))])\n",
    "\n",
    "\n",
    "\n",
    "train_x = train[:, 0]\n",
    "train_y = train[:, 1]\n",
    "\n",
    "batch_size = 1000\n",
    "epochs = 3\n",
    "\n",
    "model = make_model(input_vocab, labels_vocab, embedding_size=200, hidden_size=200, dropout=0.4)\n",
    "model.fit_generator(batch_generator(train_x, train_y, input_vocab, labels_vocab, batch_size),\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=len(train_x) / batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor=\"acc\", patience=5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Use `model.evaluate()`, not `model.evaluate_generator()`, which is incredibly slow since it pushes one sample through the network at a time. On the other hand, `model.evaluate()` allows for a large batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38123/38123 [==============================] - 1s 26us/step\n",
      "Loss: 0.6172433609311805 Acc: 0.8793510425235568\n"
     ]
    }
   ],
   "source": [
    "# Columns of original words and modernized words\n",
    "dev_words = dev[:, 0]\n",
    "dev_labels = dev[:, 1]\n",
    "\n",
    "# Vectorized inputs and labels\n",
    "dev_x = []\n",
    "dev_y = []\n",
    "\n",
    "for word, normalized_word in zip(dev_words, dev_labels):\n",
    "    word = START + word[0:10] + END\n",
    "    normalized_word = START + normalized_word[0:10] + END\n",
    "    dev_x.append(vectorize_sequence(word, input_vocab))\n",
    "    dev_y.append([one_hot_encode_label(label, labels_vocab) for label in normalized_word])\n",
    "pad_length = len(max(dev_x + dev_y, key=lambda x: len(x)))\n",
    "dev_x = pad_sequences(dev_x, pad_length, input_vocab[PAD])\n",
    "dev_y = pad_sequences(dev_y, pad_length, one_hot_encode_label(PAD, labels_vocab))\n",
    "dev_x = np.array(dev_x)\n",
    "dev_y = np.array(dev_y)\n",
    "\n",
    "loss, acc = model.evaluate(dev_x, dev_y, batch_size=1000, verbose=1)\n",
    "\n",
    "print('Loss:', loss, 'Acc:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation After Removing PAD Characters\n",
    "\n",
    "Dev accuracy is relatively good (around 75%). However, there are a lot of padding tokens in the character sequences that the model outputs. For example, the output for the word \"en\" is \"enआआआआआआआआआआआआआआआआआआआआआआआआ\" because all words are padded to the maximum length. We need to remove all of the padding tokens in order to get a more useful accuracy metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "शdemasसआआआआआ\n",
      "Accuracy after removing all padding characters: 0.6223014977834903\n"
     ]
    }
   ],
   "source": [
    "# A list of possible output characters\n",
    "labels_list = list(labels_vocab.keys())\n",
    "\n",
    "predicted_words = []\n",
    "# Each prediction is something like a 26 x 57 array, where 26 is the number of characters in the word and 57 is the size of the vocabulary\n",
    "dev_predictions = model.predict(dev_x, batch_size = 1000)\n",
    "for word_vector in dev_predictions:\n",
    "    word = \"\"\n",
    "    for character_num in range(len(word_vector)):\n",
    "        # Find the index of the character in the vocabulary with highest probability according to the model\n",
    "        vocab_index = np.argmax(word_vector[character_num])\n",
    "        # Convert that index back into a character and append it to the current word\n",
    "        character = labels_list[vocab_index]\n",
    "        word += character\n",
    "    predicted_words.append(word)\n",
    "print(predicted_words[0])\n",
    "\n",
    "# Remove all PAD characters in each word\n",
    "predicted_words = [word.replace(PAD, \"\") for word in predicted_words]\n",
    "predicted_words = [word.replace(START, \"\") for word in predicted_words]\n",
    "predicted_words = [word.replace(END, \"\") for word in predicted_words]\n",
    "\n",
    "dev_labels_truncated = [word[0:10] for word in dev_labels]\n",
    "\n",
    "results = np.column_stack((predicted_words, dev_labels_truncated))\n",
    "\n",
    "# Calculate accuracy as the percentage of exact matches between the model output (without PAD) and the labels\n",
    "accuracy = np.mean([1 if result[0] == result[1] else 0 for result in results])\n",
    "print(f\"Accuracy after removing all padding characters: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a4",
   "language": "python",
   "name": "a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
