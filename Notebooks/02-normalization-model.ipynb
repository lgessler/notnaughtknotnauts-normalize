{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics, optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "# Define unknown, pad, start, and end characters as Hindi letters since they won't be seen in the data\n",
    "UNK = 'अ'\n",
    "PAD = 'आ'\n",
    "START = 'श'\n",
    "END = 'स'\n",
    "\n",
    "dataset = pd.read_csv(r\"../data/processed_dataset.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Character Vocabulary\n",
    "\n",
    "1. Get two lists, one containing all of the characters used in the original documens and one containing all of the characters used in the modernized documents\n",
    "2. Add the special `UNK`, `PAD`, `START`, and `END` characters to each list.\n",
    "3. Create a vocabulary (i.e. a dictionary) mapping each character to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original tokens are in the first column and modernized tokens are in the second column\n",
    "original = list(dataset[:, 0])\n",
    "modernized = list(dataset[:, 1])\n",
    "\n",
    "input_characters = list(set((character for word in original for character in word)))\n",
    "input_characters += [UNK, PAD, START, END]\n",
    "input_characters = sorted(input_characters)\n",
    "input_vocab = {character:index for index, character in enumerate(input_characters)}\n",
    "\n",
    "labels_characters = list(set((character for word in modernized for character in word)))\n",
    "labels_characters += [UNK, PAD, START, END]\n",
    "labels_characters = sorted(labels_characters)\n",
    "labels_vocab = {character:index for index, character in enumerate(labels_characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization and Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequence(seq, vocab):\n",
    "    \"\"\"Takes a sequence of words and returns a sequence of integers.\"\"\"\n",
    "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
    "    return [vocab[tok] for tok in seq]\n",
    "\n",
    "\n",
    "def unvectorize_sequence(seq, vocab):\n",
    "    \"\"\"Takes a sequence of integers and returns a sequence of words.\"\"\"\n",
    "    vocab_words = list(vocab.keys())\n",
    "    return [vocab_words[i] for i in seq]\n",
    "\n",
    "\n",
    "def one_hot_encode_label(character, labels):\n",
    "    \"\"\"One-hot encodes a character.\"\"\"\n",
    "    vec = [1.0 if label == character else 0.0 for label in labels]\n",
    "    return np.array(vec)\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_length, pad_value):\n",
    "    \"\"\"Takes a batch of sequences of different lengths and pads them with the PAD character so that they are all the same length.\"\"\"\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        if len(sequence) < pad_length:\n",
    "            sequences[i] = sequence + ([pad_value] * (pad_length - len(sequence)))\n",
    "    return sequences\n",
    "\n",
    "def batch_generator(data, labels, vocab, labels_vocab, batch_size=1):\n",
    "    \"\"\"Generates a batch of samples for training.\"\"\"\n",
    "    while True:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for word, normalized_word in zip(data, labels):\n",
    "            word = START + word + END\n",
    "            normalized_word = START + normalized_word + END\n",
    "            batch_x.append(vectorize_sequence(word, vocab))\n",
    "            batch_y.append([one_hot_encode_label(character, labels_vocab) for character in normalized_word])\n",
    "            if len(batch_x) >= batch_size:\n",
    "                # Pad Sequences in batch to same length\n",
    "                pad_length = len(max(batch_x + batch_y, key=lambda x: len(x)))\n",
    "                batch_x = pad_sequences(batch_x, pad_length, vocab[PAD])\n",
    "                batch_y = pad_sequences(batch_y, pad_length, one_hot_encode_label(PAD, labels_vocab))\n",
    "                yield np.array(batch_x), np.array(batch_y)\n",
    "                batch_x = []\n",
    "                batch_y = []    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_vocab, labels_vocab, embedding_size, hidden_size, dropout):\n",
    "    \"\"\"Builds and returns a Keras model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(input_vocab.keys()), embedding_size))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Bidirectional(LSTM(hidden_size, return_sequences=True)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(TimeDistributed(Dense(len(labels_vocab.keys()), activation='softmax')))\n",
    "\n",
    "    adadelta = optimizers.Adadelta(clipnorm=1.0)\n",
    "    model.compile(optimizer=adadelta, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "305/304 [==============================] - 19s 61ms/step - loss: 1.3263 - acc: 0.7176\n",
      "Epoch 2/5\n",
      "305/304 [==============================] - 17s 56ms/step - loss: 0.7361 - acc: 0.8317\n",
      "Epoch 3/5\n",
      "305/304 [==============================] - 17s 57ms/step - loss: 0.6054 - acc: 0.8819\n",
      "Epoch 4/5\n",
      "305/304 [==============================] - 17s 56ms/step - loss: 0.5525 - acc: 0.8966\n",
      "Epoch 5/5\n",
      "305/304 [==============================] - 17s 56ms/step - loss: 0.5269 - acc: 0.9031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1feeb796198>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Split dataset into three parts: 80% training, 10% dev, 10% test\n",
    "train, dev, test = np.split(dataset, [math.floor(.8*len(dataset)), math.floor(.9*len(dataset))])\n",
    "\n",
    "train_x = train[:, 0]\n",
    "train_y = train[:, 1]\n",
    "\n",
    "batch_size = 1000\n",
    "epochs = 5\n",
    "\n",
    "model = make_model(input_vocab, labels_vocab, embedding_size=100, hidden_size=10, dropout=0.3)\n",
    "model.fit_generator(batch_generator(train_x, train_y, input_vocab, labels_vocab, batch_size),\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=len(train_x) / batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor=\"acc\", patience=5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Use `model.evaluate()`, not `model.evaluate_generator()`, which is incredibly slow since it pushes one sample through the network at a time. On the other hand, `model.evaluate()` allows for a large batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38123/38123 [==============================] - 1s 21us/step\n",
      "Loss: 1.1750560234967788 Acc: 0.7642741429231332\n"
     ]
    }
   ],
   "source": [
    "# Columns of original words and modernized words\n",
    "dev_words = dev[:, 0]\n",
    "dev_labels = dev[:, 1]\n",
    "\n",
    "# Vectorized inputs and labels\n",
    "dev_x = []\n",
    "dev_y = []\n",
    "\n",
    "for word, normalized_word in zip(dev_words, dev_labels):\n",
    "    normalized_word = START + normalized_word + END\n",
    "    dev_x.append(vectorize_sequence(word, input_vocab))\n",
    "    dev_y.append([one_hot_encode_label(label, labels_vocab) for label in normalized_word])\n",
    "pad_length = len(max(dev_x + dev_y, key=lambda x: len(x)))\n",
    "dev_x = pad_sequences(dev_x, pad_length, input_vocab[PAD])\n",
    "dev_y = pad_sequences(dev_y, pad_length, one_hot_encode_label(PAD, labels_vocab))\n",
    "dev_x = np.array(dev_x)\n",
    "dev_y = np.array(dev_y)\n",
    "\n",
    "loss, acc = model.evaluate(dev_x, dev_y, batch_size=10000, verbose=1)\n",
    "\n",
    "print('Loss:', loss, 'Acc:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a4",
   "language": "python",
   "name": "a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
