{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics, optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "# Define unknown, pad, start, and end characters as Hindi letters since they won't be seen in the data\n",
    "UNK = 'अ'\n",
    "PAD = 'आ'\n",
    "START = 'श'\n",
    "END = 'स'\n",
    "\n",
    "# Set TensorFlow logging level to not output warnings\n",
    "from tensorflow import logging\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "# Read dataset generated from Notebook 01\n",
    "dataset = pd.read_csv(r\"../data/processed_dataset.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Character Vocabulary\n",
    "\n",
    "1. Get two lists, one containing all of the characters used in the original documens and one containing all of the characters used in the modernized documents\n",
    "2. Add the special `UNK`, `PAD`, `START`, and `END` characters to each list.\n",
    "3. Create a vocabulary (i.e. a dictionary) mapping each character to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original tokens are in the first column and modernized tokens are in the second column\n",
    "original = list(dataset[:, 0])\n",
    "modernized = list(dataset[:, 1])\n",
    "\n",
    "input_characters = list(set((character for word in original for character in word)))\n",
    "input_characters += [UNK, PAD, START, END]\n",
    "input_characters = sorted(input_characters)\n",
    "input_vocab = {character:index for index, character in enumerate(input_characters)}\n",
    "\n",
    "labels_characters = list(set((character for word in modernized for character in word)))\n",
    "labels_characters += [UNK, PAD, START, END]\n",
    "labels_characters = sorted(labels_characters)\n",
    "labels_vocab = {character:index for index, character in enumerate(labels_characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization and Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequence(seq, vocab):\n",
    "    \"\"\"Takes a sequence of words and returns a sequence of integers.\"\"\"\n",
    "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
    "    return [vocab[tok] for tok in seq]\n",
    "\n",
    "\n",
    "def unvectorize_sequence(seq, vocab):\n",
    "    \"\"\"Takes a sequence of integers and returns a sequence of words.\"\"\"\n",
    "    vocab_words = list(vocab.keys())\n",
    "    return [vocab_words[i] for i in seq]\n",
    "\n",
    "\n",
    "def one_hot_encode_label(character, labels):\n",
    "    \"\"\"One-hot encodes a character.\"\"\"\n",
    "    vec = [1.0 if label == character else 0.0 for label in labels]\n",
    "    return np.array(vec)\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_length, pad_value):\n",
    "    \"\"\"Takes a batch of sequences of different lengths and pads them with the PAD character so that they are all the same length.\"\"\"\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        if len(sequence) < pad_length:\n",
    "            sequences[i] = sequence + ([pad_value] * (pad_length - len(sequence)))\n",
    "    return sequences\n",
    "\n",
    "def batch_generator(data, labels, vocab, labels_vocab, batch_size=1):\n",
    "    \"\"\"Generates a batch of samples for training.\"\"\"\n",
    "    while True:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for word, normalized_word in zip(data, labels):\n",
    "            word = START + word + END\n",
    "            normalized_word = START + normalized_word + END\n",
    "            batch_x.append(vectorize_sequence(word, vocab))\n",
    "            batch_y.append([one_hot_encode_label(character, labels_vocab) for character in normalized_word])\n",
    "            if len(batch_x) >= batch_size:\n",
    "                # Pad Sequences in batch to same length\n",
    "                pad_length = len(max(batch_x + batch_y, key=lambda x: len(x)))\n",
    "                batch_x = pad_sequences(batch_x, pad_length, vocab[PAD])\n",
    "                batch_y = pad_sequences(batch_y, pad_length, one_hot_encode_label(PAD, labels_vocab))\n",
    "                yield np.array(batch_x), np.array(batch_y)\n",
    "                batch_x = []\n",
    "                batch_y = []    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_vocab, labels_vocab, embedding_size, hidden_size, dropout):\n",
    "    \"\"\"Builds and returns a Keras model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(input_vocab.keys()), embedding_size))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Bidirectional(LSTM(hidden_size, return_sequences=True)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(TimeDistributed(Dense(len(labels_vocab.keys()), activation='softmax')))\n",
    "\n",
    "    adadelta = optimizers.Adadelta(clipnorm=1.0)\n",
    "    model.compile(optimizer=adadelta, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5338/5337 [==============================] - 161s 30ms/step - loss: 0.6056 - acc: 0.8788\n",
      "Epoch 2/3\n",
      "5338/5337 [==============================] - 162s 30ms/step - loss: 0.4986 - acc: 0.9007\n",
      "Epoch 3/3\n",
      "5338/5337 [==============================] - 165s 31ms/step - loss: 0.4733 - acc: 0.9049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20863a26f98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Split dataset into three parts: 70% training, 15% dev, 15% test\n",
    "train, dev, test = np.split(dataset, [math.floor(.7*len(dataset)), math.floor(.85*len(dataset))])\n",
    "\n",
    "train_x = train[:, 0]\n",
    "train_y = train[:, 1]\n",
    "\n",
    "batch_size = 50\n",
    "epochs = 3\n",
    "\n",
    "model = make_model(input_vocab, labels_vocab, embedding_size=200, hidden_size=500, dropout=0.5)\n",
    "model.fit_generator(batch_generator(train_x, train_y, input_vocab, labels_vocab, batch_size),\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=len(train_x) / batch_size, max_queue_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Use `model.evaluate()`, not `model.evaluate_generator()`, which is incredibly slow since it pushes one sample through the network at a time. On the other hand, `model.evaluate()` allows for a large batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57184/57184 [==============================] - 5s 82us/step\n",
      "Loss: 0.05682550839168787 Acc: 0.995196343735736\n"
     ]
    }
   ],
   "source": [
    "# Columns of original words and modernized words\n",
    "dev_words = dev[:, 0]\n",
    "dev_labels = dev[:, 1]\n",
    "\n",
    "# Vectorized inputs and labels\n",
    "dev_x = []\n",
    "dev_y = []\n",
    "\n",
    "for word, normalized_word in zip(dev_words, dev_labels):\n",
    "    word = START + normalized_word + END\n",
    "    normalized_word = START + normalized_word + END\n",
    "    dev_x.append(vectorize_sequence(word, input_vocab))\n",
    "    dev_y.append([one_hot_encode_label(label, labels_vocab) for label in normalized_word])\n",
    "pad_length = len(max(dev_x + dev_y, key=lambda x: len(x)))\n",
    "dev_x = pad_sequences(dev_x, pad_length, input_vocab[PAD])\n",
    "dev_y = pad_sequences(dev_y, pad_length, one_hot_encode_label(PAD, labels_vocab))\n",
    "dev_x = np.array(dev_x)\n",
    "dev_y = np.array(dev_y)\n",
    "\n",
    "loss, acc = model.evaluate(dev_x, dev_y, batch_size=1000, verbose=1)\n",
    "\n",
    "print('Loss:', loss, 'Acc:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation After Removing PAD Characters\n",
    "\n",
    "Dev accuracy is relatively good (around 75%). However, there are a lot of padding tokens in the character sequences that the model outputs. For example, the output for the word \"en\" is \"enआआआआआआआआआआआआआआआआआआआआआआआआ\" because all words are padded to the maximum length. We need to remove all of the padding tokens in order to get a more useful accuracy metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after removing all padding characters: 0.8896369613878008\n"
     ]
    }
   ],
   "source": [
    "# A list of possible output characters\n",
    "labels_list = list(labels_vocab.keys())\n",
    "\n",
    "predicted_words = []\n",
    "# Each prediction is something like a 26 x 57 array, where 26 is the number of characters in the word and 57 is the size of the vocabulary\n",
    "dev_predictions = model.predict(dev_x, batch_size = 1000)\n",
    "for word_vector in dev_predictions:\n",
    "    word = \"\"\n",
    "    for character_num in range(len(word_vector)):\n",
    "        # Find the index of the character in the vocabulary with highest probability according to the model\n",
    "        vocab_index = np.argmax(word_vector[character_num])\n",
    "        # Convert that index back into a character and append it to the current word\n",
    "        character = labels_list[vocab_index]\n",
    "        word += character\n",
    "    predicted_words.append(word)\n",
    "\n",
    "# Remove all PAD characters in each word and START and END tokens\n",
    "predicted_words = [word.replace(PAD, \"\") for word in predicted_words]\n",
    "predicted_words = [word.replace(START, \"\") for word in predicted_words]\n",
    "predicted_words = [word.replace(END, \"\") for word in predicted_words]\n",
    "results = np.column_stack((predicted_words, dev_labels))\n",
    "\n",
    "# Calculate accuracy as the percentage of exact matches between the model output (without PAD) and the labels\n",
    "accuracy = np.mean([1 if result[0] == result[1] else 0 for result in results])\n",
    "print(f\"Accuracy after removing all padding characters: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Suppose that throughout the corpus, word `x` maps to 4 words according to the following distribution: `[a:1, b:23, c:14, d:3]`. A naive baseline would be to always map word `x` to `b`. Below is that baseline built for all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.7824391438164522\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary, each entry for a word will be a counter\n",
    "word_map = {}\n",
    "for index, word in enumerate(train_x):\n",
    "    if word not in word_map:\n",
    "        word_map[word] = Counter()\n",
    "        word_map[word][train_y[index]] += 1\n",
    "    else:\n",
    "        word_map[word][train_y[index]] += 1\n",
    "    \n",
    "baseline_predicted_words = []\n",
    "for word in dev_words:\n",
    "    # For each original word, predict its most common mapping \n",
    "    if word in word_map:\n",
    "        predicted_word = word_map[word].most_common(1)[0][0]\n",
    "    else:\n",
    "        predicted_word = UNK\n",
    "    baseline_predicted_words.append(predicted_word)\n",
    "\n",
    "baseline_results = np.column_stack((baseline_predicted_words, dev_labels))\n",
    "baseline_accuracy = np.mean([1 if result[0] == result[1] else 0 for result in baseline_results])\n",
    "print(f\"Baseline accuracy: {baseline_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = np.column_stack((dev_words, dev_labels, baseline_predicted_words, predicted_words))\n",
    "\n",
    "df = pd.DataFrame(all_results, columns=[\"Original Word (Input)\", \" Modernized Word (Ground Truth)\", \"Naive Baseline\", \"Model Prediction\"])\n",
    "df.to_csv(r\"../Data/results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a4",
   "language": "python",
   "name": "a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
