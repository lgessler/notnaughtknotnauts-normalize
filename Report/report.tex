%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage{authblk}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Orthographic Normalization of Historical Texts}


\author[1]{Pryce Bevan}
\author[2]{Luke Gessler}
\author[1]{Michael Kranzlein}
\affil[1]{Georgetown University\\ Department of Computer Science}
\affil[2]{Georgetown University\\ Department of Linguistics}
\affil[ ]{\{\tt pwb8, lg876, mmk119\}@georgetown.edu}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We explore several model architectures for historical text normalization based on word types. At the word type level, we observe that a baseline imposes an upper bound on model performance on word types seen in training data. Accordingly, we implement an LSTM model and a transformer model to predict unseen word types and incorporate the baseline's predictions for seen word types. We test these hybrid models on historical Spanish texts and achieve results that are on par with state-of-the-art models. We also note that the LSTM model outperforms the transformer model, though this is likely due to unresolved issues with our transformer implementation.
\end{abstract}

\section{Introduction}
In many cases, researchers require or prefer to work with standardized data. In this work, we examine historical text normalization (also called canonicalization)---``the problem of translating historical documents written in the absence of modern spelling conventions and making them amenable to search by today’s scholars, processable by natural language processing models, and readable to laypeople" \cite{bollmann_multi-task_2018}. \citeauthor{bollmann_multi-task_2018} explicitly mention spelling conventions, but this is a standardization problem that extends beyond modernizing orthography. More comprehensive approaches to historical text normalization also address syntactic and semantic aspects of language, as word order changes, phrases fall out of style, new punctuation conventions are adopted, and word meanings change over time, even if the orthography doesn't. These facets of historical text normalization, however, are more difficult problems to address. We address only orthographic normalization in this work.

\subsection{Motivating the Normalization Problem}
We experiment with Spanish data because it is readily available, but we propose Coptic as a strong motivating example of the importance of historical text normalization. In work done on Coptic, \citeauthor{schroeder_coptic_2013} discuss the features of a normalized text, including “word segmentation and sentence segmentation based on modern editorial standards; standardized spelling of words throughout the text...; punctuation based on modern standards; removal of ancient strokes, punctuation, and/or accents; standardization of abbreviations” \cite{schroeder_coptic_2013}. This type of normalization performs a critical role in a manuscript-to-analysis pipeline \cite{zeldes_nlp_2016}. In short, there are many analyses digital humanists wish to pursue that orthographic irregularities are an obstacle to.

\subsection{Our Research Goals}
At the outset of this project, we aimed to achieve results on par with current state-of-the-art methods for orthographic historical text normalization. While we cannot conduct an apples-to-apples comparison with recent work (since we use a different dataset in a different language), we find generally similar results using a bidirectional LSTM recurrent neural network on Spanish data from the past few centuries \cite{graves_framewise_2005}. We attempted a transformer-based approach, but were not able to succeed in getting this model to achieve strong performance \cite{vaswani_attention_2017}. This may be due to a flawed implementation or the use of a dataset that is too small.

\section{Related Work}
Historical text normalization has received renewed attention as neural approaches continue to gain traction in the NLP community. Over the past ten years, the state of the art has seen an evolution from Hidden Markov Models and rule-based models to simple neural networks to more context-aware recurrent neural networks. Most recently, there is a shift toward attention and transformers.

\subsection{HMM, Levenshtein Edit Distance, and Rule-Based Methods}
In 2010, \citeauthor{jurish_more_2010} described a Hidden Markov Model approach to incorporate context, that is, to allow a token's normalization to be decided on the basis of not just the one token, but the total context of the token in question as well as its neighbors to the left and right. This model achieved high precision and recall.

\citeauthor{jurish_comparing_2010}, in addition to \citeauthor{pettersson_normalisation_2013}, have also both experimented with Levenshtein edit distance as a useful factor for determining the best candidate normalization. Informally, Levenshtein edit distance is the number of changes required to make one string equal to another \cite{levenshtein_binary_1966}. 

Rule-based methods have been around for a long time and remain an efficient approach for decent performance. \cite{bollmann_rule-based_2011, pettersson_spelling_2016, zeldes_nlp_2016, schneider_comparing_2017}. but more recently, state-of-the-art performance tends to come from neural approaches.

\subsection {Attention and Multi-Task Learning}
In 2017, \citeauthor{bollmann_learning_2017} explored multi-task learning in the context of historical text normalization. They found that the utility of the attention could be achieved by learning the auxiliary task of pronouncing, that is, they constructed a model that jointly learned the orthography and a grapheme-to-phoneme mapping. In 2019, \citeauthor{bollmann_few-shot_2019} expanded upon this work by adding additional auxiliary tasks and  A similar group of authors expanded on this work.

\section{Implementation}
Our code is available on GitHub at \url{https://github.com/lgessler/notnaughtknotnauts-normalize}.

\subsection{Dataset}
Many widely-used corpora in the literature were unfortunately not available. In some cases, it wasn't even clear exactly which corpus had been used (e.g. in \citet{pettersson_spelling_2016}). In others, the corpus was not easily obtainable. The Corpus of Early English Correspondence\footnote{\url{http://www.helsinki.fi/varieng/CoRD/corpora/CEEC/}}, for example, is only available on CD-ROM after submission of a payment via post to Europe. In the end, we chose to use the only suitable and freely-available corpus we could find, the Post Scriptum corpus \cite{vaamonde_post_2014}. 

The Post Scriptum corpus contains epistolary texts in Spanish and Portuguese, ranging from the 16th century to the 19th century in origin. These documents are unpublished and represent a diverse set of social backgrounds. For our experiments, we use only the Spanish texts, totaling 2368 documents with around 400,000 tokens. Each original document is included in its original orthographic form, and is also provided in modern orthography, which was manually produced by a human annotator. Our models ingest an original token as input and predict the best modern equivalent.

\begin{figure}[h]
\begin{quotation}
\noindent \textit{por me hazer md me ebye el bonete q conpre aqui}
\end{quotation}
\begin{quote}
\textit{Por me hacer merced, me envíe el bonete que compré aquí.}
\caption{A sample of unnormalized text with its normalized version from the Post Scriptum corpus.}
\end{quote}
\end{figure}

Some preprocessing was necessary in order to get the document-pairs into the form that we wanted them in: a list of tokens, both $n$ tokens long, where each token at index $i$ in either document is ``the same'' token. First, all metadata was removed from the documents, leaving only the body of the text. Next, text was lowercase normalized, and all punctuation was removed.\footnote{This primarily affected the modernized writings, as the original orthographies did not contain much punctuation.} Then, the text was whitespace-tokenized.

At this point, we might sometimes be left with neatly even lists of tokens, but unfortunately, the same tokenization was not always reproduced exactly during the normalization process: apparently, some tokens in the original document were in a many-to-one relationship with tokens in the normalized document ($t_{\text{orig},i} \rightarrow t_{\text{norm},i}, \ldots, t_{\text{norm},i+k}$), or vice versa ($t_{\text{orig},i}, \ldots, t_{\text{orig},i+k} \rightarrow t_{\text{norm},i}$). This could have arisen for a variety of reasons: mistranscriptions, the expansion of multi-word abbreviations, differences in spacing conventions, etc. 

In an effort to mitigate this as much as possible, we work with only the documents whose original and normalized versions are the same length. It's important to note that while this does not guarantee correct one-to-one alignments (one document might ``get ahead'' early on then get caught back up to towards the end), a manual observation of several thousand token pairs indicated generally good alignments. 

Once the data was preprocessed, we used an 80/10/10 split for training, validation, and testing.

\subsection{Baseline}
The baseline we used to evaluate our systems against simply takes the most commonly observed normalized token for a given original token. I.e., at each index $i$ in the document being normalized, we take: 

\[ \hat{t}_{\text{norm},i} = \arg\max_{x} \text{freq}(t_\text{orig,i}, x) \]

\noindent where the $\text{freq}$ function returns the number of occurrences of every token-pair in the entire corpus. If $\text{freq}$ is 0, the baseline system produces a special unknown token for unseen inputs.

Although simple, the baseline performs well, and can be thought of as an upper bound for the performance of any system performing token-by-token normalization, under the assumption that all inputs will have been seen during training: as long as the training corpus is sufficiently big, it is highly likely that for any given input token, the most common output token in the testing corpus will also be the most common output token in the training corpus. However, a statistical system is still worth pursuing, because the baseline is powerless if an input token has not been seen before, and it is likely that any unseen tokens will have structure a statistical system would likely be able to take advantage of.

\subsection{LSTM Model}
Our first statistical system was an LSTM-based model implemented in Keras. It consists of an embedding layer, a bidirectional LSTM layer, and a softmax layer, with dropout layers in between them. The sequence presented to the LSTM is that of the characters in the token, with special characters added for the start and end of the word, as well as any padding characters necessary to make the token fit in its batch.

\subsection{Transformer Model}
Our second statistical system was a transformer-based model. It was implemented using AllenNLP. The encoder was a \texttt{StackedSelfAttentionEncoder}, provided in the standard distribution of AllenNLP. The decoder was a modified version of AllenNLP’s \texttt{SimpleSeq2Seq} predictor that was extended so it could calculate accuracy in addition to loss. This extension, which seems like it should have been quite simple, actually took a lot of time, as AllenNLP’s abstractions make implementing another metric for a model a complicated matter, requiring code changes in multiple places. More detail will be presented in the discussion, but we were ultimately unable to get satisfactory results with the transformer model.

\section{Results}
We evaluated our performance using the standard metric in the literature for normalization: character-based accuracy. Only ``real'' characters were considered when calculating the score: special tokens for padding and marking the beginning and end of a token were ignored. Accuracy was measured in a held-out test corpus.


Following another convention in the literature, we tested both the baseline system and our statistical systems independently, and also tested a hybrid system that would use the baseline if the token had been seen before, and otherwise use the statistical model if the token had not been seen before. These were our best results after several dozen experiments, with the LSTM system hyperparameters being an embedding size of 300, and an LSTM hidden size of 100:

\begin{center}
\begin{tabular}{|l|c|}\hline
System & Accuracy \\\hline
Baseline & 85.42\% \\\hline
LSTM only & 84.88\% \\\hline
LSTM + Baseline hybrid & 90.24\% \\\hline
Transformer & 17.42\% \\\hline
\end{tabular}
\end{center}

We considered whether training and testing only on individual centuries might have helped performance, the idea being that there might be different orthographic alternations in each century, and that these century-specific alternations might even be contradictory. However, when we trained and tested on individual centuries, we found that performance did not improve or suffer. It’s difficult to know exactly why this was, but it’s possible that there were only a negligible number of century-specific alternations, or that there were a significant number of century-specific alternations but that degradations caused by smaller amount of training data for each century counterbalanced any improvements that might have been had.

We also attempted to give the LSTM model some additional context, hoping that it would be able to help in the cases where a single unnormalized token could map to potentially many normalized tokens. We added a parameter that modified the input sequence so that it would include not just the input token but also $n$ tokens to the left and right of the token, separated by special characters to represent the token boundary. We found that this had no effect for $n=1$, and that it caused severe degradations for $n>1$. 

The degradation is easily explainable: the expansion in input size increases the dimensionality of the model by quite a bit, and the reason why this kind of context isn't helping is probably that if there \emph{is} a word in the sentence that could help disambiguate an input token, then it is quite likely that it won't be within such a short range of the token. For instance, suppose an unnormalized verb could correspond to potentially many normalized forms because it was written without diacritics historically and gained diacritics in modern orthography that indicate agreement with its subject based on number, gender, etc. Occasionally the subject would be within this $n$ token distance, but it would often not be, and so context would not help.\footnote{And the situation would be worse for languages where subjects do not typically appear next to the verb phrase, e.g. Hindi, where the word order is SOV.}

\section{Discussion}
Direct comparison of our results to any other system was not possible, as there was no record we could find of another system being used with the Post Scriptum corpus. However, even with only our scores, we can speculate on how well we have done. 

The LSTM-only system comes very close to the baseline’s performance, which as discussed above can be thought of as an upper bound on performance for seen tokens. The difference is only $\sim$0.5\%. Given this, it wouldn't seem a huge leap to claim that the LSTM probably succeeded in capturing the majority of the orthographic alternations.

The failure of the transformer system is difficult to explain. Frankly, since none of us understands how the model works very well, it is not even clear whether it is possible at all for a transformer to succeed in a task like this. Assuming that it is possible, there are several other possible explanations. First, perhaps none of our hyperparameter choices were sensible. Second, perhaps we simply didn't have enough training data. (Recall that the corpus had only 400,000 tokens.) Third, perhaps there was an implementation error in either the system itself or how we implemented accuracy. Strangely, accuracy was seen to decrease as loss decreased for the transformer system. One possible reason why this might be is that accuracy was only calculated over ``real'' characters while loss was calculated for all characters, including padding characters. Even so, the same could be said for the LSTM model, where accuracy behaved as expected. 

\section{Future Work}
The biggest remaining question is what kind of errors were to blame for the remaining 10\% accuracy. Unfortunately we did not have time to conduct an extensive error analysis, but there are several obvious places to look. 

First, our na\"ive alignment strategy (assume that in each document-pair the indices for every token are the same) is, as we know, sometimes failing to properly pair off corresponding tokens. It would be a good idea to investigate more sophisticated heuristics to ensure proper alignments. Techniques like sequence matching and applying Levenshtein distance, as discussed in several of the related works, could be used to compare token pairs and better guarantee their relatedness, at the cost of some additional preprocessing time. Not only would yield higher-quality data, but it would allow us to obtain crude, but probably helpful, sentence alignments: the original documents do not have reliable sentence delimiters, but if token-level alignment were reasonably good, we could use the periods from the modernized documents to recover sentence boundaries in the original text. 

Having somewhat sentence boundaries would also allow us to structure this problem as a machine translation task. Applying a machine translation approach blindly would probably not succeed very well, since this framing would lose several useful constraints that we know should hold (e.g. that tokens should never be swapped in order during the normalization process), but it would allow us to get the context that our token-level sequence to sequence approach lacks. A first step in this direction would be to conduct an error analysis and verify that the anticipated one-to-many normalization case is actually a significant source of error.

In addition, in future work, we would further evaluate the use of transformers and other attention-based approaches, as it seems likely that transformers can actually do better and that we are just doing something wrong in our implementation at the moment.

Finally, it would be interesting to see how well our approach works on corpora in different languages, including Portuguese, which is included in the Post Scriptum corpus. Spanish is SVO, has some but not too much inflection, and has relatively strict word order. Our approach might not generalize well to highly-inflected or agglutinative languages like Plains Cree or Turkish, where out-of-vocabulary items would be much more common, and our system, unmodified, would have to begin to serve as a morphological parser and presumably suffer for it.

\section*{Acknowledgments}
We thank the attendees of the Georgetown EMNLP final project poster session for their thoughtful feedback.

\bibliography{report}
\bibliographystyle{acl_natbib}

\end{document}